{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8559dfd-b5b9-46bd-910a-57bc15765242",
      "metadata": {
        "id": "b8559dfd-b5b9-46bd-910a-57bc15765242"
      },
      "source": [
        "# LangChain QA\n",
        "\n",
        "All code comes from [LangChain docs](langchain.readthedocs.io)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7688495-ef79-4831-95bc-8c77eeb9b97d",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7688495-ef79-4831-95bc-8c77eeb9b97d",
        "outputId": "77aaa13a-c9bf-45c7-8356-c012086b94ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (4.65.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (9.1.0)\n",
            "Requirement already satisfied: tiktoken==0.3.3 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (0.3.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai chromadb tiktoken pypdf pytube youtube-transcript-api pytube aspose-words ffmpeg fpdf speech_recognition\n",
        "!pip install ibm_watson\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "OPENAI_API_KEY = getpass.getpass(\"Enter OpenAI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEFQcqnGYXv0",
        "outputId": "5cc0f293-9697-478f-d507-fea39fc5d2ed"
      },
      "id": "gEFQcqnGYXv0",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Video and Audio"
      ],
      "metadata": {
        "id": "_tKNUkHHY7ix"
      },
      "id": "_tKNUkHHY7ix"
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from ibm_watson import SpeechToTextV1\n",
        "from ibm_watson.websocket import RecognizeCallback, AudioSource\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "from pytube import YouTube\n",
        "import aspose.words as aw\n",
        "\n",
        "def audio2text(video, audio):\n",
        "  # Video to Audio\n",
        "  command = f'ffmpeg -i {video} -ab 160k -ar 44100 -vn {audio}'\n",
        "  subprocess.call(command, shell=True)\n",
        "\n",
        "  # Audio to Text\n",
        "  model = whisper.load_model(\"base\")\n",
        "  result = model.transcribe(\"audio.wav\")\n",
        "\n",
        "  print(result)\n",
        "  # Writing to .txt File\n",
        "  with open('transcript.txt', 'w') as file:\n",
        "  for i in result['text']:\n",
        "    print(i, end='')\n",
        "    file.write(i)\n",
        "\n",
        "  # TXT to PDF\n",
        "  doc = aw.Document(\"transcript.txt\")\n",
        "  doc.save(\"transcript.pdf\", aw.SaveFormat.PDF)\n",
        "  print(\"Saved text to PDF.\")\n",
        "\n",
        "def download_youtube_video(url, file_name):\n",
        "    try:\n",
        "        youtube = YouTube(url)\n",
        "        video = youtube.streams.first()\n",
        "        video.download(filename=file_name)\n",
        "        print(f\"Video downloaded successfully as {file_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "r5tFQmKb4M1n"
      },
      "id": "r5tFQmKb4M1n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_url = input(\"Youtube URL: \") # Replace with the YouTube video URL\n",
        "file_name = \"test.mp4\"  # Replace with desired file name\n",
        "download_youtube_video(youtube_url, file_name.split('/')[-1])\n",
        "audio2text(file_name, \"audio.wav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL5X6FAreyq6",
        "outputId": "4df7a75e-7945-47e1-a6dc-fb765297d08c"
      },
      "id": "tL5X6FAreyq6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Youtube URL: https://www.youtube.com/watch?v=HbY51mVKrcE&t=179s\n",
            "Video downloaded successfully as test.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \" Hey YouTube, in this video I'm going to show you how you can quickly convert any audio into text using the free open source package in Python called whisper. I'm going to show I installed it, show an example of how I ran it and compare it to an existing library. So starting off you'll probably want to go to the whisper get hub repository that we're looking at here and they give instructions on how you can install it. Now one thing to keep in mind when you pip install just the name whisper it's not going to install the right version. We want to install from this get repository. So just take this pip install command and run it in your environment that you're running Python. And they also mentioned here that you need FFN peg installed. There's some instructions to do it, but I already had that installed on my computer. Now that I have whisper install, let's just make some audio that I can test this on. So I'm going to say some idioms idioms are usually hard for models to understand even though this is just speech to text. This will be kind of fun. I would love to be on cloud nine as a one trick pony that wouldn't hurt a fly. I'd be like a fish out of water and as fit as a fiddle to be under the weather. Let's save this off. Save it as a wave. They do have instructions for how we could run this just straight from the command line once it's installed. I'm going to show you how to use the Python API which they show here. So it's really simple. We just import whisper. Then we're going to create our model which is we're going to load the model that's called base. And then just using this model object we run transcribe on our audio file. So I named it idioms. Let's use the wave version. We want this to return the result. Now I noticed when I ran this before I get this error because of kuda's half tensor and float tensor. It was able to solve this so that's something to keep in mind. If it doesn't work for you you might need to set floating point 16 to fault. And you can see after it's run here it detected the language already as English. And then this result object has a few different methods in them but what we want to get inside of this is just the text. We could see that it looks like the result is good. I would love to be on cloud nine as a one trick pony that wouldn't hurt a fly. I'd be like a fish out of water and this it did mess up a little bit this fish out of water in as fit as a fiddle. Maybe I didn't say it clearly enough. Another thing to know is when you first run this it's going to have to download the base model. So you might see a progress bar going across and you'll have to download that model. And it says when you run this transcribe it's actually taking 30 second chunks of your audio. File and running predictions on it. Now there's also another approach that you can take which is a lower level approach where you actually create the model and then you create the audio object and pattern trim this. What this will do is just make sure that this audio chunk is only 30 seconds or it'll pat it with 30 seconds since that's the length the model expects to have as input. Then it's making a log mouse spectra gram. It's detecting the language and we can decode here and provide a lot more options if we wanted to. If I run this cell again get this error which I now can set in the decoding options F P 16 equals faults. And actually this time it looks like it got everything correct. I'd be like a fish out of water and it's fit as a fiddle. So that's it for whisper. I just want to compare it to an existing type of model. In a popular library for doing this is the speech recognition library. The way we run the speech recognition library is we import it and then create this recognize our object which we then can load our audio file with. After that you can take the recognize our object and there are a few different recognizing methods for that and we're going to use the Google recognize and let's see what the result is. So it looks like it didn't add any punctuation and the cloud nine is different. I would love to be on cloud nine as a one trick pony that wouldn't hurt a fly. But the one thing to keep in mind is that this is actually using the Google speech recognition API the whisper library you actually have the model downloaded and it's yours to use. I do also recommend you take a look at the whisper paper which was released with this code. They also go into detail about how the model was trained and the architecture that it's used. The first whisper does work on a bunch of different languages. The performance they say varies based on the language so you can go here on the GitHub repo where they have a plot showing which languages actually performs best for the bars here smaller is better and larger means it performs worse. So still pretty impressive the number of languages that this model works on.\", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 9.64, 'text': \" Hey YouTube, in this video I'm going to show you how you can quickly convert any audio into text using the free open source package in Python called whisper.\", 'tokens': [50364, 1911, 3088, 11, 294, 341, 960, 286, 478, 516, 281, 855, 291, 577, 291, 393, 2661, 7620, 604, 6278, 666, 2487, 1228, 264, 1737, 1269, 4009, 7372, 294, 15329, 1219, 26018, 13, 50846], 'temperature': 0.0, 'avg_logprob': -0.18968131861735865, 'compression_ratio': 1.6470588235294117, 'no_speech_prob': 0.022048410028219223}, {'id': 1, 'seek': 0, 'start': 9.64, 'end': 15.0, 'text': \" I'm going to show I installed it, show an example of how I ran it and compare it to an existing library.\", 'tokens': [50846, 286, 478, 516, 281, 855, 286, 8899, 309, 11, 855, 364, 1365, 295, 577, 286, 5872, 309, 293, 6794, 309, 281, 364, 6741, 6405, 13, 51114], 'temperature': 0.0, 'avg_logprob': -0.18968131861735865, 'compression_ratio': 1.6470588235294117, 'no_speech_prob': 0.022048410028219223}, {'id': 2, 'seek': 0, 'start': 15.0, 'end': 24.0, 'text': \" So starting off you'll probably want to go to the whisper get hub repository that we're looking at here and they give instructions on how you can install it.\", 'tokens': [51114, 407, 2891, 766, 291, 603, 1391, 528, 281, 352, 281, 264, 26018, 483, 11838, 25841, 300, 321, 434, 1237, 412, 510, 293, 436, 976, 9415, 322, 577, 291, 393, 3625, 309, 13, 51564], 'temperature': 0.0, 'avg_logprob': -0.18968131861735865, 'compression_ratio': 1.6470588235294117, 'no_speech_prob': 0.022048410028219223}, {'id': 3, 'seek': 2400, 'start': 24.0, 'end': 30.0, 'text': \" Now one thing to keep in mind when you pip install just the name whisper it's not going to install the right version.\", 'tokens': [50364, 823, 472, 551, 281, 1066, 294, 1575, 562, 291, 8489, 3625, 445, 264, 1315, 26018, 309, 311, 406, 516, 281, 3625, 264, 558, 3037, 13, 50664], 'temperature': 0.0, 'avg_logprob': -0.13776589269223422, 'compression_ratio': 1.7598566308243728, 'no_speech_prob': 0.014566631987690926}, {'id': 4, 'seek': 2400, 'start': 30.0, 'end': 39.0, 'text': \" We want to install from this get repository. So just take this pip install command and run it in your environment that you're running Python.\", 'tokens': [50664, 492, 528, 281, 3625, 490, 341, 483, 25841, 13, 407, 445, 747, 341, 8489, 3625, 5622, 293, 1190, 309, 294, 428, 2823, 300, 291, 434, 2614, 15329, 13, 51114], 'temperature': 0.0, 'avg_logprob': -0.13776589269223422, 'compression_ratio': 1.7598566308243728, 'no_speech_prob': 0.014566631987690926}, {'id': 5, 'seek': 2400, 'start': 39.0, 'end': 47.0, 'text': \" And they also mentioned here that you need FFN peg installed. There's some instructions to do it, but I already had that installed on my computer.\", 'tokens': [51114, 400, 436, 611, 2835, 510, 300, 291, 643, 479, 37, 45, 17199, 8899, 13, 821, 311, 512, 9415, 281, 360, 309, 11, 457, 286, 1217, 632, 300, 8899, 322, 452, 3820, 13, 51514], 'temperature': 0.0, 'avg_logprob': -0.13776589269223422, 'compression_ratio': 1.7598566308243728, 'no_speech_prob': 0.014566631987690926}, {'id': 6, 'seek': 2400, 'start': 47.0, 'end': 53.0, 'text': \" Now that I have whisper install, let's just make some audio that I can test this on.\", 'tokens': [51514, 823, 300, 286, 362, 26018, 3625, 11, 718, 311, 445, 652, 512, 6278, 300, 286, 393, 1500, 341, 322, 13, 51814], 'temperature': 0.0, 'avg_logprob': -0.13776589269223422, 'compression_ratio': 1.7598566308243728, 'no_speech_prob': 0.014566631987690926}, {'id': 7, 'seek': 5300, 'start': 53.0, 'end': 60.0, 'text': \" So I'm going to say some idioms idioms are usually hard for models to understand even though this is just speech to text.\", 'tokens': [50364, 407, 286, 478, 516, 281, 584, 512, 18014, 4785, 18014, 4785, 366, 2673, 1152, 337, 5245, 281, 1223, 754, 1673, 341, 307, 445, 6218, 281, 2487, 13, 50714], 'temperature': 0.0, 'avg_logprob': -0.08043400580141724, 'compression_ratio': 1.6605839416058394, 'no_speech_prob': 0.002511497586965561}, {'id': 8, 'seek': 5300, 'start': 60.0, 'end': 66.0, 'text': \" This will be kind of fun. I would love to be on cloud nine as a one trick pony that wouldn't hurt a fly.\", 'tokens': [50714, 639, 486, 312, 733, 295, 1019, 13, 286, 576, 959, 281, 312, 322, 4588, 4949, 382, 257, 472, 4282, 27342, 300, 2759, 380, 4607, 257, 3603, 13, 51014], 'temperature': 0.0, 'avg_logprob': -0.08043400580141724, 'compression_ratio': 1.6605839416058394, 'no_speech_prob': 0.002511497586965561}, {'id': 9, 'seek': 5300, 'start': 66.0, 'end': 72.0, 'text': \" I'd be like a fish out of water and as fit as a fiddle to be under the weather.\", 'tokens': [51014, 286, 1116, 312, 411, 257, 3506, 484, 295, 1281, 293, 382, 3318, 382, 257, 24553, 2285, 281, 312, 833, 264, 5503, 13, 51314], 'temperature': 0.0, 'avg_logprob': -0.08043400580141724, 'compression_ratio': 1.6605839416058394, 'no_speech_prob': 0.002511497586965561}, {'id': 10, 'seek': 5300, 'start': 72.0, 'end': 80.0, 'text': \" Let's save this off. Save it as a wave. They do have instructions for how we could run this just straight from the command line once it's installed.\", 'tokens': [51314, 961, 311, 3155, 341, 766, 13, 15541, 309, 382, 257, 5772, 13, 814, 360, 362, 9415, 337, 577, 321, 727, 1190, 341, 445, 2997, 490, 264, 5622, 1622, 1564, 309, 311, 8899, 13, 51714], 'temperature': 0.0, 'avg_logprob': -0.08043400580141724, 'compression_ratio': 1.6605839416058394, 'no_speech_prob': 0.002511497586965561}, {'id': 11, 'seek': 8000, 'start': 80.0, 'end': 87.0, 'text': \" I'm going to show you how to use the Python API which they show here. So it's really simple. We just import whisper.\", 'tokens': [50364, 286, 478, 516, 281, 855, 291, 577, 281, 764, 264, 15329, 9362, 597, 436, 855, 510, 13, 407, 309, 311, 534, 2199, 13, 492, 445, 974, 26018, 13, 50714], 'temperature': 0.0, 'avg_logprob': -0.09259107979861173, 'compression_ratio': 1.61244019138756, 'no_speech_prob': 0.24824094772338867}, {'id': 12, 'seek': 8000, 'start': 87.0, 'end': 93.0, 'text': \" Then we're going to create our model which is we're going to load the model that's called base.\", 'tokens': [50714, 1396, 321, 434, 516, 281, 1884, 527, 2316, 597, 307, 321, 434, 516, 281, 3677, 264, 2316, 300, 311, 1219, 3096, 13, 51014], 'temperature': 0.0, 'avg_logprob': -0.09259107979861173, 'compression_ratio': 1.61244019138756, 'no_speech_prob': 0.24824094772338867}, {'id': 13, 'seek': 8000, 'start': 93.0, 'end': 104.0, 'text': \" And then just using this model object we run transcribe on our audio file. So I named it idioms. Let's use the wave version.\", 'tokens': [51014, 400, 550, 445, 1228, 341, 2316, 2657, 321, 1190, 1145, 8056, 322, 527, 6278, 3991, 13, 407, 286, 4926, 309, 18014, 4785, 13, 961, 311, 764, 264, 5772, 3037, 13, 51564], 'temperature': 0.0, 'avg_logprob': -0.09259107979861173, 'compression_ratio': 1.61244019138756, 'no_speech_prob': 0.24824094772338867}, {'id': 14, 'seek': 10400, 'start': 104.0, 'end': 113.0, 'text': \" We want this to return the result. Now I noticed when I ran this before I get this error because of kuda's half tensor and float tensor.\", 'tokens': [50364, 492, 528, 341, 281, 2736, 264, 1874, 13, 823, 286, 5694, 562, 286, 5872, 341, 949, 286, 483, 341, 6713, 570, 295, 350, 11152, 311, 1922, 40863, 293, 15706, 40863, 13, 50814], 'temperature': 0.0, 'avg_logprob': -0.12365292258884596, 'compression_ratio': 1.6426116838487972, 'no_speech_prob': 0.4733166992664337}, {'id': 15, 'seek': 10400, 'start': 113.0, 'end': 120.0, 'text': \" It was able to solve this so that's something to keep in mind. If it doesn't work for you you might need to set floating point 16 to fault.\", 'tokens': [50814, 467, 390, 1075, 281, 5039, 341, 370, 300, 311, 746, 281, 1066, 294, 1575, 13, 759, 309, 1177, 380, 589, 337, 291, 291, 1062, 643, 281, 992, 12607, 935, 3165, 281, 7441, 13, 51164], 'temperature': 0.0, 'avg_logprob': -0.12365292258884596, 'compression_ratio': 1.6426116838487972, 'no_speech_prob': 0.4733166992664337}, {'id': 16, 'seek': 10400, 'start': 120.0, 'end': 132.0, 'text': \" And you can see after it's run here it detected the language already as English. And then this result object has a few different methods in them but what we want to get inside of this is just the text.\", 'tokens': [51164, 400, 291, 393, 536, 934, 309, 311, 1190, 510, 309, 21896, 264, 2856, 1217, 382, 3669, 13, 400, 550, 341, 1874, 2657, 575, 257, 1326, 819, 7150, 294, 552, 457, 437, 321, 528, 281, 483, 1854, 295, 341, 307, 445, 264, 2487, 13, 51764], 'temperature': 0.0, 'avg_logprob': -0.12365292258884596, 'compression_ratio': 1.6426116838487972, 'no_speech_prob': 0.4733166992664337}, {'id': 17, 'seek': 13200, 'start': 132.0, 'end': 139.0, 'text': \" We could see that it looks like the result is good. I would love to be on cloud nine as a one trick pony that wouldn't hurt a fly.\", 'tokens': [50364, 492, 727, 536, 300, 309, 1542, 411, 264, 1874, 307, 665, 13, 286, 576, 959, 281, 312, 322, 4588, 4949, 382, 257, 472, 4282, 27342, 300, 2759, 380, 4607, 257, 3603, 13, 50714], 'temperature': 0.0, 'avg_logprob': -0.10334281380294907, 'compression_ratio': 1.7714285714285714, 'no_speech_prob': 0.3385019898414612}, {'id': 18, 'seek': 13200, 'start': 139.0, 'end': 148.0, 'text': \" I'd be like a fish out of water and this it did mess up a little bit this fish out of water in as fit as a fiddle. Maybe I didn't say it clearly enough.\", 'tokens': [50714, 286, 1116, 312, 411, 257, 3506, 484, 295, 1281, 293, 341, 309, 630, 2082, 493, 257, 707, 857, 341, 3506, 484, 295, 1281, 294, 382, 3318, 382, 257, 24553, 2285, 13, 2704, 286, 994, 380, 584, 309, 4448, 1547, 13, 51164], 'temperature': 0.0, 'avg_logprob': -0.10334281380294907, 'compression_ratio': 1.7714285714285714, 'no_speech_prob': 0.3385019898414612}, {'id': 19, 'seek': 13200, 'start': 148.0, 'end': 153.0, 'text': \" Another thing to know is when you first run this it's going to have to download the base model.\", 'tokens': [51164, 3996, 551, 281, 458, 307, 562, 291, 700, 1190, 341, 309, 311, 516, 281, 362, 281, 5484, 264, 3096, 2316, 13, 51414], 'temperature': 0.0, 'avg_logprob': -0.10334281380294907, 'compression_ratio': 1.7714285714285714, 'no_speech_prob': 0.3385019898414612}, {'id': 20, 'seek': 13200, 'start': 153.0, 'end': 161.0, 'text': \" So you might see a progress bar going across and you'll have to download that model. And it says when you run this transcribe it's actually taking 30 second chunks of your audio.\", 'tokens': [51414, 407, 291, 1062, 536, 257, 4205, 2159, 516, 2108, 293, 291, 603, 362, 281, 5484, 300, 2316, 13, 400, 309, 1619, 562, 291, 1190, 341, 1145, 8056, 309, 311, 767, 1940, 2217, 1150, 24004, 295, 428, 6278, 13, 51814], 'temperature': 0.0, 'avg_logprob': -0.10334281380294907, 'compression_ratio': 1.7714285714285714, 'no_speech_prob': 0.3385019898414612}, {'id': 21, 'seek': 16100, 'start': 161.0, 'end': 175.0, 'text': \" File and running predictions on it. Now there's also another approach that you can take which is a lower level approach where you actually create the model and then you create the audio object and pattern trim this.\", 'tokens': [50364, 26196, 293, 2614, 21264, 322, 309, 13, 823, 456, 311, 611, 1071, 3109, 300, 291, 393, 747, 597, 307, 257, 3126, 1496, 3109, 689, 291, 767, 1884, 264, 2316, 293, 550, 291, 1884, 264, 6278, 2657, 293, 5102, 10445, 341, 13, 51064], 'temperature': 0.0, 'avg_logprob': -0.13683446248372397, 'compression_ratio': 1.6768558951965065, 'no_speech_prob': 0.09135166555643082}, {'id': 22, 'seek': 16100, 'start': 175.0, 'end': 185.0, 'text': \" What this will do is just make sure that this audio chunk is only 30 seconds or it'll pat it with 30 seconds since that's the length the model expects to have as input.\", 'tokens': [51064, 708, 341, 486, 360, 307, 445, 652, 988, 300, 341, 6278, 16635, 307, 787, 2217, 3949, 420, 309, 603, 1947, 309, 365, 2217, 3949, 1670, 300, 311, 264, 4641, 264, 2316, 33280, 281, 362, 382, 4846, 13, 51564], 'temperature': 0.0, 'avg_logprob': -0.13683446248372397, 'compression_ratio': 1.6768558951965065, 'no_speech_prob': 0.09135166555643082}, {'id': 23, 'seek': 18500, 'start': 185.0, 'end': 196.0, 'text': \" Then it's making a log mouse spectra gram. It's detecting the language and we can decode here and provide a lot more options if we wanted to.\", 'tokens': [50364, 1396, 309, 311, 1455, 257, 3565, 9719, 6177, 424, 21353, 13, 467, 311, 40237, 264, 2856, 293, 321, 393, 979, 1429, 510, 293, 2893, 257, 688, 544, 3956, 498, 321, 1415, 281, 13, 50914], 'temperature': 0.0, 'avg_logprob': -0.183873314606516, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.31281325221061707}, {'id': 24, 'seek': 18500, 'start': 196.0, 'end': 208.0, 'text': ' If I run this cell again get this error which I now can set in the decoding options F P 16 equals faults. And actually this time it looks like it got everything correct.', 'tokens': [50914, 759, 286, 1190, 341, 2815, 797, 483, 341, 6713, 597, 286, 586, 393, 992, 294, 264, 979, 8616, 3956, 479, 430, 3165, 6915, 36090, 13, 400, 767, 341, 565, 309, 1542, 411, 309, 658, 1203, 3006, 13, 51514], 'temperature': 0.0, 'avg_logprob': -0.183873314606516, 'compression_ratio': 1.502415458937198, 'no_speech_prob': 0.31281325221061707}, {'id': 25, 'seek': 20800, 'start': 208.0, 'end': 216.0, 'text': \" I'd be like a fish out of water and it's fit as a fiddle. So that's it for whisper. I just want to compare it to an existing type of model.\", 'tokens': [50364, 286, 1116, 312, 411, 257, 3506, 484, 295, 1281, 293, 309, 311, 3318, 382, 257, 24553, 2285, 13, 407, 300, 311, 309, 337, 26018, 13, 286, 445, 528, 281, 6794, 309, 281, 364, 6741, 2010, 295, 2316, 13, 50764], 'temperature': 0.0, 'avg_logprob': -0.0898335756257523, 'compression_ratio': 1.6807511737089202, 'no_speech_prob': 0.33544254302978516}, {'id': 26, 'seek': 20800, 'start': 216.0, 'end': 230.0, 'text': ' In a popular library for doing this is the speech recognition library. The way we run the speech recognition library is we import it and then create this recognize our object which we then can load our audio file with.', 'tokens': [50764, 682, 257, 3743, 6405, 337, 884, 341, 307, 264, 6218, 11150, 6405, 13, 440, 636, 321, 1190, 264, 6218, 11150, 6405, 307, 321, 974, 309, 293, 550, 1884, 341, 5521, 527, 2657, 597, 321, 550, 393, 3677, 527, 6278, 3991, 365, 13, 51464], 'temperature': 0.0, 'avg_logprob': -0.0898335756257523, 'compression_ratio': 1.6807511737089202, 'no_speech_prob': 0.33544254302978516}, {'id': 27, 'seek': 23000, 'start': 230.0, 'end': 239.0, 'text': \" After that you can take the recognize our object and there are a few different recognizing methods for that and we're going to use the Google recognize and let's see what the result is.\", 'tokens': [50364, 2381, 300, 291, 393, 747, 264, 5521, 527, 2657, 293, 456, 366, 257, 1326, 819, 18538, 7150, 337, 300, 293, 321, 434, 516, 281, 764, 264, 3329, 5521, 293, 718, 311, 536, 437, 264, 1874, 307, 13, 50814], 'temperature': 0.0, 'avg_logprob': -0.08579042222764757, 'compression_ratio': 1.645933014354067, 'no_speech_prob': 0.7348601818084717}, {'id': 28, 'seek': 23000, 'start': 239.0, 'end': 248.0, 'text': \" So it looks like it didn't add any punctuation and the cloud nine is different. I would love to be on cloud nine as a one trick pony that wouldn't hurt a fly.\", 'tokens': [50814, 407, 309, 1542, 411, 309, 994, 380, 909, 604, 27006, 16073, 293, 264, 4588, 4949, 307, 819, 13, 286, 576, 959, 281, 312, 322, 4588, 4949, 382, 257, 472, 4282, 27342, 300, 2759, 380, 4607, 257, 3603, 13, 51264], 'temperature': 0.0, 'avg_logprob': -0.08579042222764757, 'compression_ratio': 1.645933014354067, 'no_speech_prob': 0.7348601818084717}, {'id': 29, 'seek': 24800, 'start': 248.0, 'end': 258.0, 'text': \" But the one thing to keep in mind is that this is actually using the Google speech recognition API the whisper library you actually have the model downloaded and it's yours to use.\", 'tokens': [50364, 583, 264, 472, 551, 281, 1066, 294, 1575, 307, 300, 341, 307, 767, 1228, 264, 3329, 6218, 11150, 9362, 264, 26018, 6405, 291, 767, 362, 264, 2316, 21748, 293, 309, 311, 6342, 281, 764, 13, 50864], 'temperature': 0.0, 'avg_logprob': -0.07924708837195288, 'compression_ratio': 1.6339285714285714, 'no_speech_prob': 0.7134995460510254}, {'id': 30, 'seek': 24800, 'start': 258.0, 'end': 270.0, 'text': \" I do also recommend you take a look at the whisper paper which was released with this code. They also go into detail about how the model was trained and the architecture that it's used.\", 'tokens': [50864, 286, 360, 611, 2748, 291, 747, 257, 574, 412, 264, 26018, 3035, 597, 390, 4736, 365, 341, 3089, 13, 814, 611, 352, 666, 2607, 466, 577, 264, 2316, 390, 8895, 293, 264, 9482, 300, 309, 311, 1143, 13, 51464], 'temperature': 0.0, 'avg_logprob': -0.07924708837195288, 'compression_ratio': 1.6339285714285714, 'no_speech_prob': 0.7134995460510254}, {'id': 31, 'seek': 27000, 'start': 270.0, 'end': 289.0, 'text': ' The first whisper does work on a bunch of different languages. The performance they say varies based on the language so you can go here on the GitHub repo where they have a plot showing which languages actually performs best for the bars here smaller is better and larger means it performs worse.', 'tokens': [50364, 440, 700, 26018, 775, 589, 322, 257, 3840, 295, 819, 8650, 13, 440, 3389, 436, 584, 21716, 2361, 322, 264, 2856, 370, 291, 393, 352, 510, 322, 264, 23331, 49040, 689, 436, 362, 257, 7542, 4099, 597, 8650, 767, 26213, 1151, 337, 264, 10228, 510, 4356, 307, 1101, 293, 4833, 1355, 309, 26213, 5324, 13, 51314], 'temperature': 0.0, 'avg_logprob': -0.1340494155883789, 'compression_ratio': 1.6577777777777778, 'no_speech_prob': 0.28996211290359497}, {'id': 32, 'seek': 27000, 'start': 289.0, 'end': 295.0, 'text': ' So still pretty impressive the number of languages that this model works on.', 'tokens': [51314, 407, 920, 1238, 8992, 264, 1230, 295, 8650, 300, 341, 2316, 1985, 322, 13, 51614], 'temperature': 0.0, 'avg_logprob': -0.1340494155883789, 'compression_ratio': 1.6577777777777778, 'no_speech_prob': 0.28996211290359497}], 'language': 'en'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method-2 LangChain"
      ],
      "metadata": {
        "id": "yHhK3RloxHKe"
      },
      "id": "yHhK3RloxHKe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4793f8d6-bf79-4513-8a31-06e209852a59",
      "metadata": {
        "tags": [],
        "id": "4793f8d6-bf79-4513-8a31-06e209852a59"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd5ba580-2c29-450b-bb9c-edd301a7da4d",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd5ba580-2c29-450b-bb9c-edd301a7da4d",
        "outputId": "3a0c15eb-2f2b-448d-89f0-6b6724e63c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Q: What did the fish say when it hit the wall?\n",
            "A: Dam!\n"
          ]
        }
      ],
      "source": [
        "## OpenAI Check\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "llm = OpenAI()\n",
        "print(llm(\"tell me a joke\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc8f34b4-b121-4c9a-8425-7a8fa8fea367",
      "metadata": {
        "id": "bc8f34b4-b121-4c9a-8425-7a8fa8fea367"
      },
      "source": [
        "# load_qa_chain\n",
        "\n",
        "Loads a chain that you can use to do QA over a set of documents, but it uses ALL of those documents.\n",
        "\n",
        "chain_type=\"stuff\" will not work because the number of tokens exceeds the limit. We can try other chain types like \"map_reduce\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c343c502-093d-4161-9f14-9fc52a8b725c",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "c343c502-093d-4161-9f14-9fc52a8b725c",
        "outputId": "39ede3e7-196a-4e2b-8cd9-f23c2bc4f89b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The tools used here are pip install streamlit, Langchain, open AI, Wikipedia, Chroma DB, tick token, Wikipedia API wrapper, title memory buffer, script memory buffer, and wiki pdf research.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# load document\n",
        "loader = PyPDFLoader(\"transcript.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "### For multiple documents\n",
        "# loaders = [....]\n",
        "# documents = []\n",
        "# for loader in loaders:\n",
        "#     documents.extend(loader.load())\n",
        "\n",
        "chain = load_qa_chain(llm=OpenAI(), chain_type=\"map_reduce\")\n",
        "query = \"what all the tools were used?\"\n",
        "chain.run(input_documents=documents, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136ffbdf-a0c1-4da7-9080-e7d7e47b57d1",
      "metadata": {
        "tags": [],
        "id": "136ffbdf-a0c1-4da7-9080-e7d7e47b57d1"
      },
      "source": [
        "# VectorstoreIndexCreator\n",
        "\n",
        "VectorstoreIndexCreator is a wrapper for the above logic.\n",
        "\n",
        "Source:\n",
        "- https://python.langchain.com/en/latest/modules/chains/getting_started.html\n",
        "- https://github.com/hwchase17/langchain/blob/master/langchain/indexes/vectorstore.py#L21-L74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d85b40-37aa-43b0-a29e-cda49f972425",
      "metadata": {
        "tags": [],
        "id": "f6d85b40-37aa-43b0-a29e-cda49f972425"
      },
      "outputs": [],
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    # split the documents into chunks\n",
        "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0),\n",
        "    # select which embeddings we want to use\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    # use Chroma as the vectorestore to index and search embeddings\n",
        "    vectorstore_cls=Chroma\n",
        ").from_loaders([loader])\n",
        "query = \"what is the total number of AI publications?\"\n",
        "index.query(llm=OpenAI(), question=query, chain_type=\"map_reduce\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}